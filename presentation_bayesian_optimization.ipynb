{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction To Bayesian Optimization \n",
    "\n",
    "Hitarth Choubisa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2> <center> Outline  </center> </h2>\n",
    "\n",
    "* Motivation: Why go beyond Supervised and Unsupervised Learning?\n",
    "\n",
    "* Drawbacks of Supervised Learning algorithms\n",
    "\n",
    "* Gaussian Process Regression (GPR)\n",
    "\n",
    "* Bayesian Optimization (BO) using GPR\n",
    "\n",
    "* Python for BO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2> <center> Bayesian Optimization  </center> </h2>\n",
    "\n",
    "* Can ML tell us **the next best experiment?**\n",
    "* Can ML tell us where we have uncertainty in our experimental space?\n",
    "* Bayesian Optimization allows us to answer both questions!\n",
    "* Algorithms we have covered thus far lack this ability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plot_gp(mu, cov, X, X_train=None, Y_train=None, samples=[]):\n",
    "    X = X.ravel()\n",
    "    mu = mu.ravel()\n",
    "    uncertainty = 1.96 * np.sqrt(np.diag(cov))\n",
    "    \n",
    "    plt.fill_between(X, mu + uncertainty, mu - uncertainty, alpha=0.1)\n",
    "    plt.plot(X, mu, label='Mean')\n",
    "    for i, sample in enumerate(samples):\n",
    "        plt.plot(X, sample, lw=1, ls='--', label=f'Sample {i+1}')\n",
    "    if X_train is not None:\n",
    "        plt.plot(X_train, Y_train, 'rx')\n",
    "    plt.legend()\n",
    "\n",
    "def plot_gp_2D(gx, gy, mu, X_train, Y_train, title, i):\n",
    "    ax = plt.gcf().add_subplot(1, 2, i, projection='3d')\n",
    "    ax.plot_surface(gx, gy, mu.reshape(gx.shape), cmap=cm.coolwarm, linewidth=0, alpha=0.2, antialiased=False)\n",
    "    ax.scatter(X_train[:,0], X_train[:,1], Y_train, c=Y_train, cmap=cm.coolwarm)\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2> <center> What does uncertainty mean?</center> </h2>\n",
    "\n",
    "![](pics_for_a3md/missing_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2> <center> Supervised Learning  </center> </h2>\n",
    "\n",
    "* Supervised Machine Learning: Given input, predict (Continuous or Discrete) output \n",
    "\n",
    "![](pics_for_a3md/sup_learning.png)\n",
    "\n",
    "* Logisitic Regression, Support Vector Machines, Random Forests etc..\n",
    "* Problems: These algorithms to not work well with:\n",
    "    * A large input space\n",
    "    * Small datasets\n",
    "* The solution: Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2> <center> Gaussian Process Regression  </center> </h2>\n",
    "\n",
    "* There are two ways to approach ML: \n",
    "    * Probablistic Approach: (like Naive Bayes)\n",
    "    * Non-probablistic approach: (Logistic Regression, Perceptron, SVM, etc.)\n",
    "\n",
    "\n",
    "* Naive Bayes: Every dimension of input(X) is a Random Variable; y is the target\n",
    "    $$P(X|y)=\\prod_{n=1}^d P(x_n|y)$$\n",
    "    \n",
    "$X=[x_1,x_2,...,x_d]$\n",
    "\n",
    "* and we try to find the class, y, that maximixes the function\n",
    "$$P(y|X) = P(x|y)P(y)$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian Processes\n",
    "\n",
    "* What if we model every datapoint as Random Variable?\n",
    "    $$P(y|X) = P(y_1,...,y_N|[x_1,x_2,...,x_N];[w_1,...,w_k])$$\n",
    "    $X: \\text{ \\{Collection of all the input pairs\\}}$, \n",
    "     N: number of datapoints, $w_i$: parameters of the ML model\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probabilities: Joint and Conditional\n",
    "* A random variable X can be defined given its probability distribution $p(X)$\n",
    "* Given two random variables X and Y, their joint probability is $p(X,Y)$ and it satisfies $\\int_{R} \\int_{R}p(x,y)dx dy = 1$ and $p(X,Y) \\geq 0$\n",
    "* Also, given a joint distribution, the conditional probability distribution can be defined as:\n",
    "$$p(X|Y) = \\frac{p(X,Y)}{p(Y)}$$\n",
    "* We can recover the distribution for individual random variables from the joint distribution as, $$p(X) = \\int_{R}p(x,y)dy$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example of Coin Toss\n",
    "\n",
    "* Toss two coins together\n",
    "* Set of all the outcomes is: $\\{HH, TT, HT, TH\\}$\n",
    "* Let's calculate different probabilities in this case\n",
    "* $P(C_1=H,C_2=H) = P(HH) = P(C_1=H) \\cdot P(C_2=H) = 1/2 * 1/2 = 0.25 $\n",
    "\n",
    "* $P(HH) = P(HT) = P(TH) = P(TT) = \\frac{1}{4}$\n",
    "\n",
    "* $C_1$: Coin 1, $C_2$: Coin 2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Coin Toss example\n",
    "\n",
    "* To recover individual distribution:\n",
    "<br>\n",
    "$$p(X) = \\int_{R}p(x,y)dy$$\n",
    "$$P(C_1=H) = P(C_1=H,C_2=H)+P(C_1=H, C_2=T) = 1/2$$\n",
    "\n",
    "* To get the conditional distribution:\n",
    "<br>\n",
    "$$p(X|Y) = \\frac{p(X,Y)}{p(Y)}$$\n",
    "$$P(C_2=T|C_1=H) = \\frac{P(C_1=H,C_2=T)}{P(C_1=H)} = \\frac{P(HT)}{0.5} = \\frac{0.25}{0.5} = 0.5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian Process (GP)\n",
    "* **Definition**: GP is a collection of random variables (RV) such that the joint distribution of every finite subset of RVs is multivariate Gaussian\n",
    "* The multivariate Gaussian distribution is defined by a mean vector $\\mu$ and a covariance matrix $\\Sigma$. We denote this using a simple notation: $N(\\mu, \\Sigma)$. We call $\\Sigma$ co-variance matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Normal distribution\n",
    "import numpy as np\n",
    "\n",
    "x = np.linspace(-3.,3.,100)\n",
    "mu = 1\n",
    "sigma = 0.7\n",
    "y = 1/np.sqrt(2*sigma**2)*np.exp(-0.5*(x-mu)**2/sigma**2)\n",
    "plt.plot(x,y)\n",
    "plt.ylim(0,8)\n",
    "plt.vlines(mu,0,max(y),'r','dashed')\n",
    "plt.ylabel('Probability density function')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](pics_for_a3md\\g_rvs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* For a multidimensional Gaussian distribution, the entries of $\\Sigma$ dictates how fast the decay happens in different directions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Latent/Objective Function:\n",
    "* Goal of Gaussian processes: Learn the underlying multivariate distribution from training data.\n",
    "\n",
    "* Let *f* be the function we want to optimize: we call it **latent function** or **objective function**: $f(x_n) \\rightarrow y_n$\n",
    "* *f* can be anything: experimental mapping, radioactivity decay, expensive-to-evaluate mathematical function\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process\n",
    "\n",
    "* A Gaussian process is a random process where the joint distribution of a finite number of these variables $p(f(x1),…,f(xN))$ is itself Gaussian: $$p(f|X)=N(\\mu,\\Sigma)$$\n",
    "<br>\n",
    "where $f=(f(x_1),f(x_2),...,f(x_N)), \\mu=(m(x_1),m(x_2),..,m(x_N))$ and $\\Sigma_{ij}=\\kappa(x_i,x_j)$. **m** is the mean function and $\\kappa$ is called a kernel function which is used for capturing how close two input pairs are.\n",
    "\n",
    "\n",
    "* As we observe new outputs, we improve upon this: prior to posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kernels\n",
    "* Recall that in order to set up our distribution, we need to define $\\mu$ and $\\Sigma$.\n",
    "* In Gaussian processes it is often assumed that $\\mu = 0$ \n",
    "* The key step of Gaussian processes is setting up the covariance matrix $\\Sigma$:\n",
    "\n",
    "    * We do this by evaluating the kernel $\\kappa$, pairwise on all the points. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The kernel receives two points $x,x'\\in \\mathbb{R}^n$ as an input and returns a similarity measure between those points in the form of a scalar.\n",
    "* Might have seen this in context of SVMs\n",
    "* kernels allow similarity measures beyond the standard euclidean distance (L2-distance)\n",
    "* A few common kernels are shown below:\n",
    "\n",
    "![](pics_for_a3md/kernels.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def kernel(X1, X2, l=1.0, sigma_f=1.0):\n",
    "    '''\n",
    "    Isotropic squared exponential kernel. Computes \n",
    "    a covariance matrix from points in X1 and X2.\n",
    "    \n",
    "    Generating covariance matrix for training data,\n",
    "    we just give the same input twice to X1 and X2\n",
    "        \n",
    "    Args:\n",
    "        X1: Array of m points (m x d).\n",
    "        X2: Array of n points (n x d).\n",
    "\n",
    "    Returns:\n",
    "        Covariance matrix (m x n).\n",
    "    '''\n",
    "    sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\n",
    "    return sigma_f**2 * np.exp(-0.5 / l**2 * sqdist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x1 = np.linspace(-3,3,100).reshape(-1,1)\n",
    "x2 = x1.copy()\n",
    "\n",
    "outs = kernel(x1,x2)\n",
    "plt.imshow(outs)\n",
    "plt.colorbar()\n",
    "#plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## The Prior distribution p(f|X)\n",
    "* From the definition of GPR,  $$p(f|X)=N(\\mu,\\Sigma)$$\n",
    "<br>\n",
    "where $f=(f(x_1),f(x_2),...,f(x_N)), \\mu=(m(x_1),m(x_2),..,m(x_N))$ and $\\Sigma_{ij}=\\kappa(x_i,x_j)$. \n",
    "* **m** is the mean function and $\\kappa$ is a kernel function\n",
    "* Without loss of generality, we assume that means are zeros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finite number of points\n",
    "X = np.arange(-5, 5, 0.2).reshape(-1, 1)\n",
    "\n",
    "# Mean and covariance of the prior\n",
    "mu = np.zeros(X.shape)\n",
    "cov = kernel(X, X)\n",
    "\n",
    "# Draw three samples from the prior\n",
    "samples = np.random.multivariate_normal(mu.ravel(), cov, 3)\n",
    "# Each sample is a guess of what function looks like!\n",
    "# Since we haven't observed anything, all of them are equally likely.\n",
    "\n",
    "# Plot GP mean, confidence interval and samples \n",
    "plot_gp(mu, cov, X, samples=samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prior probability distribution\n",
    "* **A GP prior $p(f|X)$ can be converted to a GP posterior $p(f|X,y)$ after observing some data y.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prior to Posterior: incorporating the observations\n",
    "\n",
    "* By definition of GP, \n",
    "$$P(f|X) = N(\\mu,\\Sigma)$$ where $f = [f(x_1),...f(x_n)]$ and $X = [x_1,...,x_n]$\n",
    "\n",
    "\n",
    "\n",
    "* If we observe values of function *f* for some inputs *x*, we now denote the following\n",
    "\n",
    "    * $X$: Input points at which we made the observations\n",
    "    * $y$: Evaluation of function *f* at $X$ $\\rightarrow$ these are our observations!\n",
    "    * $X_*$: Points at which we want to know or predict the values\n",
    "    * $f_*$: The predicted output of the function at $X_*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Thus, we can write,\n",
    "$$P(y,f_*|X,X_*) \\rightarrow N(0,[[K_y,K_*],[K^T_*,K_{**}]])$$\n",
    "\n",
    "<br>\n",
    "* Here, $K_y=\\kappa(X,X)$, $K_*=\\kappa(X,X_*)$ and $K_{**}=\\kappa(X_*,X_*)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Using Marginalization and some 4 pages of maths....\n",
    "    $$p(f_*|X_*,X,y) =N(f_*|\\mu_*,\\Sigma_*)$$\n",
    "\n",
    "* Where,\n",
    "$$\\mu_*=K_*^TK^{-1}_y y$$\n",
    "$$\\Sigma_* = K_{**} - K^T_* K^{-1}_y K_*$$\n",
    "\n",
    "* An important point to observe: $y_i$ only appears in update of mean vector $\\mu$. It doesn't influence the uncertainty $\\Sigma$\n",
    "\n",
    "* Refer to [this](http://www.gaussianprocess.org/gpml/) for a detailed proof of the above relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "def posterior_predictive(X_s, X_train, Y_train, l=1.0, sigma_f=1.0, sigma_y=1e-8):\n",
    "    '''  \n",
    "    Computes the suffifient statistics of the GP posterior predictive distribution \n",
    "    from m training data X_train and Y_train and n new inputs X_s.\n",
    "    \n",
    "    Args:\n",
    "        X_s: New input locations (n x d).\n",
    "        X_train: Training locations (m x d).\n",
    "        Y_train: Training targets (m x 1).\n",
    "        l: Kernel length parameter.\n",
    "        sigma_f: Kernel vertical variation parameter.\n",
    "        sigma_y: Noise parameter.\n",
    "    \n",
    "    Returns:\n",
    "        Posterior mean vector (n x d) and covariance matrix (n x n).\n",
    "    '''\n",
    "    K = kernel(X_train, X_train, l, sigma_f) + sigma_y**2 * np.eye(len(X_train))\n",
    "    K_s = kernel(X_train, X_s, l, sigma_f)\n",
    "    K_ss = kernel(X_s, X_s, l, sigma_f) + 1e-8 * np.eye(len(X_s))\n",
    "    K_inv = inv(K)\n",
    "    \n",
    "    # Equation (4)\n",
    "    mu_s = K_s.T.dot(K_inv).dot(Y_train)\n",
    "\n",
    "    # Equation (5)\n",
    "    cov_s = K_ss - K_s.T.dot(K_inv).dot(K_s)\n",
    "    \n",
    "    return mu_s, cov_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training a Gaussian Process:\n",
    "* Training is equivalent to finding the posterior distribution given observed data (X) and inputs(y)\n",
    "\n",
    "<br>\n",
    "$$p(f_*|X_*,X,y) =N(f_*|\\mu_*,\\Sigma_*)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Noise free training data\n",
    "X_train = np.array([-4, -3, -2, -1, 1]).reshape(-1, 1)\n",
    "Y_train = np.sin(X_train)\n",
    "\n",
    "# Define the testing space we want to probe\n",
    "X = np.arange(-5, 5, 0.2).reshape(-1, 1)\n",
    "\n",
    "# Compute mean and covariance of the posterior predictive distribution\n",
    "mu_s, cov_s = posterior_predictive(X, X_train, Y_train)\n",
    "\n",
    "samples = np.random.multivariate_normal(mu_s.ravel(), cov_s, 3)\n",
    "# get rid of the different samples\n",
    "plot_gp(mu_s, cov_s, X, X_train=X_train, Y_train=Y_train, samples=samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Consequences of GP\n",
    "* Gaussian Process Regression gives us a way to quantify uncertainty \n",
    "* We use the covariance matrix to find the standard deviation\n",
    "    * High deviation -> High uncertainity\n",
    "* How is this useful?\n",
    "    * We exploit the tradeoff between uncertainty and certainty\n",
    "    * This exploitation is the basis of Bayesian Optimization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Optimization basics\n",
    "\n",
    "* Making smart setting choices \n",
    "![](pics_for_a3md/flow_bo.png)\n",
    "\n",
    "* Based on initial or procured experimental datapoints, we use Bayesian Optimization to find new set of parameters which can help us direct our next set of experiments\n",
    "\n",
    "* These points can be based either on the maximum uncertainity or maximum potential reward: this is typically referred to as a trade off between exploitation and exploration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Optimization\n",
    "\n",
    "* Sequential design strategy for global optimization of black-box functions\n",
    "* Usually employed to optimize expensive-to-evaluate functions\n",
    "* Using information / data available to us to navigate the available exploration space\n",
    "* This is very useful for experimentalists where performing every experiment is time consuming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's define a few terms!\n",
    "* Bayesian Optimization builds a probability model of the **objective function** to find the global optimimum in a minimum number of steps\n",
    "* The **objective function** is the black-box-function we want to optimize $\\rightarrow$ mapping experimental conditions to experimental outputs. (We will discuss this further)\n",
    "* Bayesian optimization incorporates a prior belief about the objective function *f* and updates the prior with samples drawn from *f* to get a posterior that better approximates *f*.\n",
    "\n",
    "![](pics_for_a3md/gp_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Surrogate models & Uncertainty:\n",
    "* Given the input data, we want to find the points of the input space which we expect to be highly rewarding. \n",
    "* This can be done with using uncertainities:\n",
    "    * **Exploration**: Exploring regions of inputs where we have higher uncertainty\n",
    "    * **Exploitation**: Choosing points in regions where we are more certain to get guarranted rewards\n",
    "* Mathematically, uncertainity is quantified using probability. Gaussian Process (GP) helps us model uncertainties \n",
    "* Such a model is called a **Surrogate Model**\n",
    "\n",
    "* **Acqusition function** is the way we specify the trade-off between exploration and exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](pics_for_a3md/time_evolution_gp_c.png)\n",
    "* The blue region indicates the uncertainty of the output space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Acquisition function:\n",
    "\n",
    "* Bayesian optimization uses an acquisition function that directs sampling to areas where an improvement over the current best observation is likely.\n",
    "* Popular acquisition functions: maximum probability of improvement (MPI), expected improvement (EI) \n",
    "\n",
    "$$MPI(x) = \\psi(\\frac{\\mu(x)-f(x^+)-\\xi}{\\sigma(x)})$$\n",
    "\n",
    "$$EI(x) = (\\mu(x)-f(x^+)-\\xi) \\psi(\\frac{\\mu(x)-f(x^+)-\\xi}{\\sigma(x)}) + \\sigma(x) \\phi(\\frac{\\mu(x)-f(x^+)-\\xi}{\\sigma(x)})$$\n",
    "\n",
    "$$UCB(x) = \\mu(x) + \\beta \\sigma(x)$$\n",
    "\n",
    "where $\\mu(x)$ is the mean function, $\\sigma(x)$ is the variance, $\\beta$ & $\\xi$ are parameters controlling degree of exploration and $\\psi(z)$ \\& $\\phi(z)$ is cumulative distribution function of a standard Gaussian distribution $N(0,1)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Bayesian Optimization Framework:\n",
    "\n",
    "* The Bayesian optimization procedure is as follows. For t=1,2,… repeat:\n",
    "    * Find the next sampling point $x_t$ by optimizing the acquisition function over the GP: $x_t=argmax_xu(x|D_{1:t−1})$\n",
    "    * Obtain a possibly noisy sample $y_t=f(x_t)+ϵ_t$ from the objective function f.\n",
    "    * Add the sample to previous samples $D_{1:t}=\\{D_{1:t−1},(x_t,y_t)\\}$ and update the GP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Optimization in Python\n",
    "\n",
    "* Several libraries are available for performing bayesian optimization in Python\n",
    "* We will use two libraries: scikit-learn and bayesian-optimization\n",
    "* We will use scikit-learn for developing surrogate model; and the bayesian-optimization library  for the optimization\n",
    "* Lets install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!pip install bayesian-optimization\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's solve a dummy problem with Bayesian Optimization \n",
    "\n",
    "* We will choose a dummy function which we will probe\n",
    "* Using Bayesian Optimization, we will try to find the point where it achieves the maximum\n",
    "* black_box_function can return a truth table as well\n",
    "    * This is what you will use for experimental data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def black_box_function(x, y):\n",
    "    \"\"\"Function with unknown internals we wish to maximize.\n",
    "    In our case, this will be a simple ML model trained on experimental datapoints\n",
    "    \"\"\"\n",
    "    return -x ** 2 - (y - 1) ** 2 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's introduce the BayesianOptimization object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Bounded region of parameter space\n",
    "pbounds = {'x': (2, 4), 'y': (-3, 3)}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=black_box_function,\n",
    "    pbounds=pbounds,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* f is the function to be optimized and expensive to compute\n",
    "* pbounds: bounds on the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's perform the maximize operation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "optimizer.maximize(\n",
    "    init_points=2,\n",
    "    n_iter=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* init_points: Represent number of steps of random experiments we want to perform\n",
    "* n_iter: Steps of Bayesian Optimization we want to perform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's look at the point where we are expect to perform our next experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What if we don't want to train an ML model ?\n",
    "* We can specify the points we want the framework to probe\n",
    "\n",
    "* Our black_box_function can then provide the experimental data we already know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "optimizer.probe(\n",
    "    params={\"x\": 0.5, \"y\": 0.7},\n",
    "    lazy=True,\n",
    ")\n",
    "\n",
    "optimizer.probe(\n",
    "    params=[-0.3, 0.1],\n",
    "    lazy=True,\n",
    ")\n",
    "\n",
    "# Will probe only the two points specified above\n",
    "optimizer.maximize(init_points=0, n_iter=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Including several pre-defined points from experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "vals = [[0.5,0.6],[0.9,0.4],[0.3,0.88]]\n",
    "\n",
    "for i in vals:\n",
    "    optimizer.probe( params=i,lazy=True,)\n",
    "\n",
    "\n",
    "# Will probe only the two points specified above\n",
    "optimizer.maximize(init_points=0, n_iter=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Saving the trained models for later usage\n",
    "\n",
    "* Will save things in a .json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "\n",
    "logger = JSONLogger(path=\"./logs.json\")\n",
    "optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "# Results will be saved in ./logs.json\n",
    "optimizer.maximize(\n",
    "    init_points=2,\n",
    "    n_iter=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loading the progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt.util import load_logs\n",
    "\n",
    "\n",
    "new_optimizer = BayesianOptimization(\n",
    "    f=black_box_function,\n",
    "    pbounds={\"x\": (-2, 2), \"y\": (-2, 2)},\n",
    "    verbose=2,\n",
    "    random_state=7,\n",
    ")\n",
    "\n",
    "# New optimizer is loaded with previously seen points\n",
    "load_logs(new_optimizer, logs=[\"./logs.json\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do we deal with discrete variables?\n",
    "\n",
    "* Till now all the exercises were with continuous variables\n",
    "* There is no direct way to handle discrete variables but we can do some tricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def function_to_be_optimized(x, y, w):\n",
    "    d = int(w)\n",
    "    return ((x + y + d) // (1 + d)) / (1 + (x + y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimization(f=function_to_be_optimized, pbounds={'x': (-10, 10), 'y': (-10, 10), 'w': (0, 5)})\n",
    "optimizer.maximize(init_points=2,n_iter=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Can we modify the hyper-parameters of the underlying GP?\n",
    "* The answer is yes; it can be done while calling the maximize routine\n",
    "* The backend uses [sklearn's Gaussian Process Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html)\n",
    "* So, any parameter that is supported there can be given as an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "optimizer.maximize(\n",
    "    init_points=1,\n",
    "    n_iter=2,\n",
    "    # What follows are GP regressor parameters\n",
    "    alpha=1e-3,\n",
    "    n_restarts_optimizer=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Appendix:\n",
    "* Bayesian Neural Networks (BNNs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Neural Networks (BNNs)\n",
    "\n",
    "* BNN is a neural network with a prior distribution on its weights\n",
    "* The known data is used to update the posterior given our prior beliefs about the weights\n",
    "* Standard NN training via optimization is equivalent to maximum likelihood estimation (MLE) for the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maths of BNNs:\n",
    "* We are given data set $\\{(x_n,y_n)\\}$, where each data point comprises of features $x_n \\in R^D$ and output $y_n \\in R$. \n",
    "* Define the likelihood for each data point as,\n",
    "$$p(y_n|w,x_n,\\sigma^2) = N(y_n|h(x_n;w),\\sigma^2)$$\n",
    "where $h(\\cdot)$ denotes a neural network whose weights and biases form the latent variables **w**. Assume $\\sigma^2$ is known. \n",
    "* We also define the prior on the weights and biases w to be standard normal:\n",
    "$$p(w) = N(0,1)$$\n",
    "* A very easy way to run a BNN is through [Edward library](http://edwardlib.org/tutorials/bayesian-neural-network)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
